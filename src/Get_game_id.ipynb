{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the list of all games with its id number and ouput a file at `/data/game_id.csv`\n",
    "As of 11/8/2019. There are 345727 games. More information about the API can be found here https://rawg.io/apidocs and its endpoints can be found here https://api.rawg.io/docs/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import requests\n",
    "from pprint import pprint\n",
    "import os\n",
    "import csv\n",
    "from time import time\n",
    "import concurrent.futures\n",
    "import functools\n",
    "import math\n",
    "\n",
    "with open(\"../secret.json\", \"r\") as f:\n",
    "    API_KEY = json.load(f)[\"API_KEY\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multithreading\n",
    "This function is responsible for requesting pages of games (40 games per page) and save as a JSON file in `/data/game_id/`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def worker(start_index, urls_per_worker, urls, downloaded_files, headers):\n",
    "    for url in urls[start_index: start_index + urls_per_worker]:\n",
    "        if url.rsplit(\"page=\")[-1] in downloaded_files:\n",
    "            continue\n",
    "        try:\n",
    "            # Request API\n",
    "            json_data = json.loads(requests.get(url, headers=headers).text)\n",
    "\n",
    "            # Get wanted data\n",
    "            D = {game[\"id\"]: game[\"slug\"] for game in json_data[\"results\"]}\n",
    "\n",
    "            # Save data\n",
    "            page_no = int(url.split(\"page=\")[-1])\n",
    "            with open(fr\"../data/game_id/{page_no}.json\", \"w\", encoding=\"utf8\") as f:\n",
    "                json.dump(D, f)\n",
    "        except:\n",
    "            print(f\"Error with {url}\")\n",
    "\n",
    "    # Verbose notification\n",
    "    print(\n",
    "        f\"Done from {urls[start_index]} to {urls[start_index + urls_per_worker]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 16624 urls, 32 workers. Thus, each worker will request 519 urls\n"
     ]
    }
   ],
   "source": [
    "# Create data folder if not already existed\n",
    "if not os.path.exists('../data/game_id/'):\n",
    "    os.makedirs('../data/game_id/')\n",
    "\n",
    "# Make the first request to get the total amount of pages to get\n",
    "headers = {'User-Agent': 'App Name: Education purpose', }\n",
    "params = {\"key\": API_KEY, \"page_size\": 40, \"page\": 1}\n",
    "response = requests.get(rf\"https://api.rawg.io/api/games?\",\n",
    "                        headers=headers, params=params)\n",
    "json_data = json.loads(response.text)\n",
    "no_of_pages = math.ceil(json_data[\"count\"]/40)\n",
    "\n",
    "# Skip downloaded files\n",
    "downloaded_files = {file.split(\".\", 1)[0]\n",
    "                    for file in os.listdir(\"../data/game_id/\")}\n",
    "\n",
    "# Make urls\n",
    "url = response.url\n",
    "urls = [url[:-1] + str(i) for i in range(1, no_of_pages + 1)\n",
    "        if str(i) not in downloaded_files]\n",
    "\n",
    "# Set up number of workers\n",
    "max_workers = 32\n",
    "urls_per_worker = int(len(urls)/max_workers)\n",
    "start_index = range(0, len(urls), urls_per_worker)\n",
    "\n",
    "print(f\"There are {len(urls)} urls, {max_workers} workers. Thus, each worker will request {urls_per_worker} urls\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following codes apply concurrent programming to speed up the progress. 32 workers are running at the same time. Each of the workers will individually make a request. Time was reduced from ~ 4 hours to ~40 minutes for 17272 pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run all workers on all urls\n",
    "t0 = time()\n",
    "with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "    temp = functools.partial(worker,\n",
    "                             urls_per_worker=urls_per_worker,\n",
    "                             urls=urls,\n",
    "                             downloaded_files=downloaded_files,\n",
    "                             headers=headers,\n",
    "                             )\n",
    "    executor.map(temp, start_index)\n",
    "print(f\"Time taken: {time()-t0}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load each JSON file in `/data/game_id/` and write to a CSV file which is saved at `/data/game_id.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/game_id.csv\", \"w\") as f:\n",
    "    csv_file = csv.writer(f, lineterminator=\"\\n\")\n",
    "    for file in os.listdir(\"../data/game_id/\"):\n",
    "        try:\n",
    "            json_data = json.load(open(f\"../data/game_id/{file}\", \"r\"))\n",
    "        except:\n",
    "            print(file)\n",
    "        for game_id, game_name in json_data.items():\n",
    "            csv_file.writerow([game_id, game_name])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
